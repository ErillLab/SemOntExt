# ScienceOne
ScienceOne text-mining project repository


# Main goals

The main goal of the project is to be able to *generate author-sanctioned GO annotations* from *un- or semi-structured text*. That is, to develop a *domain- and/or author-targeted text-mining system* that would present users with automatic GO annotations for validation. Such annotations should be specific enough to not overwhelm users with multiple inaccurate annotations for validation. Hence, while the overall goal is to retrieve as many GO annotations as possible from the text, the main focus should be on precision, not recall. The bottom line is that it better to capture a subset of the GO annotations present in the source, and have them validated by the author, than not to capture anything at all (which is the current status quo).

A secondary goal of the project is to make this process as generic as possible. Even though the pilot targets a specific scientific domain (transcriptional regulation in bacteria), the underlying motivation is to use it as proof-of-concept, not to tailor the solution to it. Ideally, the system should be transportable to other domains without a major investment, typically understood here as a large, dedicated manual curation effort to generate a training/test corpus.

## Relevance
As far as I know, both goals are innovative in the field. Full extraction of GO annotations (as a triad consisting of gene product, GO term and evidence code) has not been implemented so far, and the development of a method that could be easily portable (yet still efficient) across domains would also be a major development.

# Intuitions and rationale
The main intuition behind the project is to take a different approach to conventional information extraction (IE) tools, which typically aim at extracting relationships through highly-tailored supervised algorithms requiring large manually-curated corpora and manually fine-tuned parameters and algorithms. The key insight here is that the main goal of such algorithms is to infer relationships that can be readily and broadly exploited in the domain of interest (bona fide relationships in high-volume document processing), whereas in this project the aim is to infer relationships that can be validated by authors on their own work.

An obvious, yet unsubstantiated, intuition of the project is that constraining the domain to the author level should somehow enhance the IE process. This is likely true for the named entity recognition (NER) step, since knowledge of the author's past work (generally referred here to as profile) and hierarchical parsing (e.g. figuring out species to constrain gene/protein names) seems likely to result in increased specificity (this has been demonstrated in [GNAT <http://gnat.sourceforge.net/>]). It is not so obvious how author profiling may enhance relation extraction (RE) among named entities.

# Target applications
The ultimate goal of the project is for such a system to be deployed in a cloud-based lab-notebook environment. This would most likely be an easier scenario for an IE system than extracting relationships from published literature. The type of evidence used in a GO statement, for instance, would be readily available as the experiment type, and the system would be able to exploit many aspects of the user profile (such as which organisms they work with, which genes they typically study, or even explicitly defined targets in a structured online workflow (i.e. reported experiment belongs to project X, which studies gene Y from organism Z in biological process W)). However, we don’t currently have such a system up, and we can only have access to a limited repertoire of such statements, manually generated from published literature. Published literature, on the other hand, provides a substantially larger corpus to operate on and to test some of the assumptions and methods devised for the ultimate application.

# Knowledge-bases
## CollecTF + GOA
Our main knowledge-base is CollecTF. CollecTF contains curated records for ~900 peer-reviewed publications. The curation process in CollecTF involves compiling data for reported transcription factor (TF)-binding sites. Hence, not all the information potentially leading to a GO annotation is captured in CollecTF. However, CollecTF records can be mined for GO annotations. In particular, a CollecTF record captures information about a TF binding a promoter and/or regulating a set of genes, together with the experimental method used to support these statements. These “TF-centric” statements can be readily converted into GO annotations. If a well-defined biological process is associated with a particular TF (e.g. transcription factor Fur coordinates the cellular response to iron starvation), it follows that genes shown to be regulated by the TF ought to participate in that biological process, a fact that can also be expressed using the GO annotation formalism. CollecTF-generated GO annotations are available at <http://collectf.org/static/collectf.gpad>. 

## GO and ECO
The other two main resources available are the GO and ECO (Evidence Ontology). The premise of the project is that one may generate a generic system for IE provided that there is at least a well-developed set of ontologies on the topic domain area. This may seem like a basic statement, but the aim here is to see if one can shift the emphasis from curated corpora (annotated text-to-relation datasets) to well-developed ontologies. In contrast to corpora, ontologies have multiple uses and are easier to develop and maintain. Hence it is much more likely that, in any given domain, appropriate ontologies will be already in use or can be developed (with payback beyond the text-mining). The underlying idea here is that ontologies provide two main types of information: 1) definitional information that can be fed to a NER system and 2) relationship information that can be fed to the RE module (e.g. in GO a term like “transcription initiation” has offspring terms, like “regulation of transcription initiation”, that identify specific relationships for the less granular term. One can use this information directly during NER (trying to capture the more granular term) or indirectly, during RE (using the terminology in the derived term to elicit the relation).

## NCBI & EBI resources
Finally, for NER of protein names we will be relying heavily on the NCBI and EBI resources. The idea is to implement a GNAT-like NER for gene/protein names, where we first identify bacterial species/strains and we then make use of the associated protein/genome records to compile a bag-of-words to look up gene instances.

# Brief review of approaches
There are three main paradigms used in IE (concerning mostly RE): supervised learning, weak/distant supervision and unsupervised learning. 

* Supervised learning performs, as expected, best, but it is heavily constrained by the manual effort devoted to curation of corpora and does not generalize well. The supervised methods differ in their machine learning inference core and in the amount of syntactical parsing done and its representation, ranging from SVMs to Conditional Random Fields on one end to bag-of-words, lexical rules, shallow and deep (dependency) syntactic parsing. 

* Distant supervision and bootstrapping are interesting alternatives, using bigger, mostly unlabelled corpora with simpler ML cores and typically only lexical or shallow syntax parsing. In distant supervision, the intuition is to use a database of relationships (e.g. FreeBase, now moved/morphing to WikiData) to go at large quantities of unlabelled text, infer relationships and test them against the relationship database. Because relationships are identified in large quantities of text, the intuition is that relevant RE features will be weighted up in the long run. Another key point is that the system will not tend to overfit, since it is not being directly tested on labelled text. In bootstrapping, a seed set of relationships is used to start a “snowball” effect. Essentially, given some relationship examples in a relationship database, one goes after text instances of these, creates possible extraction rules and detects new instances of relationships, which are then added to the database. The process then starts anew, with the new set of relationships acting as the seed.

# Proposed strategy
The problem, as stated, can obviously be subdivided into NER followed by RE (or EE, Event Extraction). Although a “classical” supervised ML approach for full GO annotation extraction could be a reasonable thesis goal, it seems unlikely that it would be easy to tailor it to leverage author-focus. The original idea behind the approach was to leverage author-focus approach, so that the “tool” could be deployed with minimal changes into any domain where there exists a well defined set of relevant ontologies. This calls for an unsupervised, or weakly supervised approach. In other words, leveraging author-focus and the fact that we have well-defined ontologies (useful for other things beyond text-mining), can we come up with a weakly/non-supervised method to efficiently extract GO annotations?

Author focus should probably be more important during NER than RE, since authors are likely to keep looking at same organisms, genes, and topics (GO terms) using same or similar techniques. Therefore, the idea is to see first whether we can effectively leverage author-focus to boost up a primarily unsupervised NER. Since we are just exploring, we’ll start with the simplest model (TF/IDF bag-of-words on both manuscripts and GO terms) and see how well (if at all, we do). Matt will be annotating direct (word) matches to GO/ECO/protein names (based on the GO/ECO records), as well as “what-a-machine-could-extract” and “what-a-person-can-extract” regions in the text. He will do this for 5 authors, annotating 5 papers per author. The idea, then, is to run the BoW model and see what it comes up with and how likely it would be to pick annotated stuff. If it looks promising, then we’ll investigate on possible (simple) patches to it (e.g. heuristics for document parsing (intro vs. results vs. M&M), bootstrapping (e.g. extending the original BoW to include neighboring words, weighted by the confidence of the original hit), etc.).

## Manuscript structural information
It would be naïve to approach the NER stage without leveraging the structured nature of scientific papers. The proposed approach is two-tiered, relying on a **first topic extraction-like pass** in which relevant GO, ECO and protein terms for a document are identified. It is in this context that the author-focus can yield enhanced results, by ranking signficant identified terms with regard to their likelihood of occurring in the author's world. This first pass would focus on specific sections of the document for different terms. For **GO terms**, the analysis would rely on *Introduction* and *Conclusions*, where authors tend to be more verbose and (we assume) mention all relevant GO terms for the paper. For **ECO terms**, the analysis would rely on the *Materials and Methods* (or namesake) section, where authors typically detail all the relevant methodology. For **gene/protein names**, the relevant species would likely be identified in the *Abstract* and *Introduction* sections.

In the **second NER pass**, having identified document-relevant GO, ECO and protein name terms, we will scan the *Results* (or *Results and Discussion*) section of the paper for instances of this limited set of terms.
